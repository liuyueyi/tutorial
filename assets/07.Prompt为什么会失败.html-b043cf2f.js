import{_ as e,V as t,W as r,a1 as a}from"./framework-094145d2.js";const o={},l=a(`<hr><h2 id="order-7title-第-3-章-prompt-为什么会失败-tag-llmcategory-llmdate-2025-12-30-12-15-07keywords-llm应用开发" tabindex="-1"><a class="header-anchor" href="#order-7title-第-3-章-prompt-为什么会失败-tag-llmcategory-llmdate-2025-12-30-12-15-07keywords-llm应用开发" aria-hidden="true">#</a> order: 7 title: 第 3 章：Prompt 为什么会失败？ tag: - LLM category: - LLM date: 2025-12-30 12:15:07 keywords: LLM应用开发</h2><p>在大模型应用开发之初，demo版、或者初版的设计一般大同小异，比如以企业知识库助手为例，第一版实现通常是这样的：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>“你是一个企业知识库助手，请根据公司文档回答用户的问题。”
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>从实际的表现来看，demo还行，但是离生产使用，总是差点意思：</p><ul><li>回答流畅（大模型的输出一般还是很可以的，至少语言组织天赋应该胜过很多像我这样比较潦草的人了🤣）</li><li>语气专业（即使回答内容有误，也是一股正经、理直气壮，这就是让开发者感到为难的地方了~）</li><li>但事实经常对不上（例如把 “3 个工作日审批” 说成 “5 个工作日”，把 “部门经理审批” 漏掉）</li></ul><p>那么问题来了：</p><blockquote><p><strong>Prompt 已经写了，为什么系统还是不可靠？</strong></p></blockquote><hr><h3 id="_3-1-一个必须先纠正的认知误区" tabindex="-1"><a class="header-anchor" href="#_3-1-一个必须先纠正的认知误区" aria-hidden="true">#</a> 3.1 一个必须先纠正的认知误区</h3><p>很多小伙伴可能会有下面这个潜意识</p><blockquote><p>“Prompt 是给模型下指令的。”</p></blockquote><p>但如果你还记得第一部分的结论，就会意识到：</p><blockquote><p><strong>LLM 并不会‘执行指令’，它只是在延续文本。</strong></p></blockquote><p>这是啥意思呢，又表明什么呢？</p><ul><li><code>Prompt</code> 不是命令（模型没有 “理解指令并执行” 的能力，它只是根据上下文概率生成下一个词）</li><li><code>Prompt</code> 是上下文的一部分（模型会把 Prompt 当作 “对话历史” 的开头，然后 “接话”）</li><li><code>Prompt</code> 的作用，是改变生成空间的形状（通过调整上下文，让模型更可能生成符合预期的内容）</li></ul><p>如果你在上下文中留下了模糊空间，大模型“自由合理地填满这些空白”，就是必然会出现的结果了</p><p>举个例子来解释下上面的说法</p><p>当用户问 “公司的病假工资怎么算？”，如果 Prompt 只说 “根据公司文档回答”，模型会怎么做？</p><p>它会先从训练数据中回忆 “一般公司的病假工资规则”（比如 “按基本工资的 80% 发放”），再尝试把这个 “常识” 套进 “公司文档” 的语境里</p><p>即使文档中明确写的是 “按全额工资发放”，模型也可能因为 “常识更熟悉” 而优先输出错误内容。</p><hr><h3 id="_3-2-企业场景下-prompt-最常见的三类失败模式" tabindex="-1"><a class="header-anchor" href="#_3-2-企业场景下-prompt-最常见的三类失败模式" aria-hidden="true">#</a> 3.2 企业场景下，Prompt 最常见的三类失败模式</h3><h4 id="失败模式一-角色边界缺失" tabindex="-1"><a class="header-anchor" href="#失败模式一-角色边界缺失" aria-hidden="true">#</a> 失败模式一：角色边界缺失</h4><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>“你是一个知识库助手。”
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>这个描述在自然语言中是清晰的，但对模型来说，信息几乎为零。</p><p>问题在于：</p><ul><li>这个角色是否可以使用常识？（比如用户问 “打印机坏了怎么办”，是否可以推荐 “重启试试” 这种通用经验？）</li><li>是否可以综合多份资料？（比如用户问 “跨部门协作流程”，是否允许整合《部门沟通规范》和《项目管理手册》的内容？）</li><li>是否允许推断 “合理但未写明” 的结论？（比如文档说 “员工入职满 1 年可休年假”，是否可以推断 “不满 1 年不可休”？）</li></ul><blockquote><p><strong>没有被否定的行为，都会被模型视为“允许”。</strong></p></blockquote><p>在企业场景中，这种 “默认允许” 往往是灾难的源头。</p><p>比如某公司文档只写了 “经理级以上可申请弹性工作”，模型可能会 “合理推断”“主管级以下不可申请”</p><p>但实际上，文档漏写了 “主管级满足条件也可申请”，导致员工权益受损。</p><hr><h4 id="失败模式二-信息来源未被工程化" tabindex="-1"><a class="header-anchor" href="#失败模式二-信息来源未被工程化" aria-hidden="true">#</a> 失败模式二：信息来源未被工程化</h4><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>“请结合你所知道的内容回答。”
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>从模型视角看：</p><ul><li>“你所知道的内容” = 训练语料（互联网上的通用知识、其他公司的制度、甚至过时的信息） + 当前上下文（可能包含的部分文档片段）</li></ul><p>而在企业知识库助手中，我们真正想要的是：</p><blockquote><p><strong>一个“被严格限制的信息子集”（即 “仅当前提供的公司内部文档”）。</strong></p></blockquote><p>这也是为什么后续 RAG 的核心价值，并不只是“补充知识”，而是： <strong>缩小生成空间。</strong></p><p>举个具体案例：</p><p>某公司 2025 年更新了差旅费标准（住宿上限从 800 元调整为 1000 元），但模型训练数据截止到 2024 年。</p><p>如果 Prompt 没有严格限制 “仅用提供的 2024 年文档”，当用户问 “出差住宿能报多少” 时，模型会优先输出训练数据中的 800 元</p><p>因为对它来说，“旧知识” 比 “新文档片段” 更 “熟悉”。</p><hr><h4 id="失败模式三-输出目标不可判定" tabindex="-1"><a class="header-anchor" href="#失败模式三-输出目标不可判定" aria-hidden="true">#</a> 失败模式三：输出目标不可判定</h4><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>“请详细说明相关内容。”
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>这类 Prompt 最大的问题不是“不清楚”，而是：</p><blockquote><p><strong>你无法客观判断它有没有答对。</strong></p></blockquote><p>详细” 是多详细？“相关” 是哪些相关？当输出结果出现时，你只能凭感觉说 “好像对” 或 “好像不对”，但无法用明确的标准验证。</p><p>一旦输出目标不可判定：</p><ul><li>系统无法测试（无法写自动化用例验证 “是否符合详细说明的要求”）</li><li>系统无法评估（无法量化 “回答准确率”，只能靠人工主观打分）</li><li>系统也无法迭代（不知道改 Prompt 后是变好还是变坏）</li></ul><p>例如，当用户问 “报销流程有几步” 时，“详细说明” 可能输出 “先填单、再审批、最后报销”（3 步），也可能输出 “填单需附发票、审批分部门和财务、报销到账约 3 天”（包含额外信息）。</p><p>这两种输出都符合 “详细”，但前者漏了关键步骤（如 “发票校验”），却无法通过明确规则判定错误。</p><hr><h3 id="_3-3-本章小结" tabindex="-1"><a class="header-anchor" href="#_3-3-本章小结" aria-hidden="true">#</a> 3.3 本章小结</h3><p>到这里，我们可以给出一个工程层面的总结：</p><blockquote><p><strong>Prompt 失败的根本原因，不是表达能力不足，而是约束设计不足。</strong></p></blockquote><p>Prompt 的核心职责不是 “告诉模型该怎么说”（比如 “用专业语气”“分点回答”），而是：</p><blockquote><p><strong>明确告诉模型：哪些内容是被允许生成的，哪些不是。</strong></p></blockquote><p>既然我们已经找到了 Prompt 失败的根源是 “约束设计不足”，那么解决方案就不再是 “优化措辞”，而是构建一套系统化的约束框架。Prompt 不应该是零散的指令集合，而需要有稳定、可复用的结构来承载这些约束</p><p>—— 这正是第 4 章要探讨的核心：如何通过工程化的结构设计，让 Prompt 真正成为可靠的 “约束系统”。</p>`,62),p=[l];function i(d,n){return t(),r("div",null,p)}const c=e(o,[["render",i],["__file","07.Prompt为什么会失败.html.vue"]]);export{c as default};
