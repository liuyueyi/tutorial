import{_ as e,V as l,W as s,$ as i,Y as n,F as t}from"./framework-b1bd8911.js";const o="/tutorial/imgs/column/llm/18-1.webp",r="/tutorial/imgs/column/llm/18-2.webp",p="/tutorial/imgs/column/llm/18-3.webp",u="/tutorial/imgs/column/llm/18-4.webp",d={},c=n('<p>在前面的章节中，我们已经逐步解决了：</p><ul><li>如何约束模型行为（Prompt）</li><li>如何管理对话与上下文（Context / Memory）</li></ul><p>但到这里，一个更根本的问题开始浮现：</p><blockquote><p><strong>如果用户的问题，本身就超出了模型的知识边界，或者企业私有知识范围，系统还能做什么？</strong></p></blockquote><p>这就是我们前面提到 RAG（Retrieval-Augmented Generation）存在的原因。</p><hr><h2 id="_12-1-先别急着实现-什么是-rag" tabindex="-1"><a class="header-anchor" href="#_12-1-先别急着实现-什么是-rag" aria-hidden="true">#</a> 12.1 先别急着实现：什么是 RAG</h2><p>在上一篇中，已经从工程的视角介绍了RAG: <code>动态构造一个“最小且相关的知识上下文”</code></p><p>我们对齐进行展开</p><blockquote><p><strong>RAG 是一种： 在模型生成之前， 通过外部检索机制， 为模型“临时补充其本不具备的知识”的系统架构。</strong></p></blockquote><p>注意几个关键词：</p><ul><li><strong>生成之前</strong></li><li><strong>外部</strong></li><li><strong>临时</strong></li><li><strong>补充</strong></li></ul><p>这意味着：</p><ul><li>知识不进入模型参数</li><li>不依赖模型“记住”</li><li>不污染长期上下文</li></ul><hr><h2 id="_12-2-rag-解决的-其实是-知识时效与边界问题" tabindex="-1"><a class="header-anchor" href="#_12-2-rag-解决的-其实是-知识时效与边界问题" aria-hidden="true">#</a> 12.2 RAG 解决的，其实是“知识时效与边界问题”</h2><p>为什么只靠模型本身一定不够？</p><p>在企业知识库助手中，典型问题包括：</p><ul><li>企业制度频繁变化</li><li>内部流程不对外公开</li><li>文档体量远超上下文窗口</li><li>法规 / 合同 / SOP 有明确版本边界</li></ul><p>这些问题有一个共同特征：</p><blockquote><p><strong>它们不是“模型不聪明”，而是“模型不可能知道”。</strong></p></blockquote><p>RAG 的核心价值就在于：</p><blockquote><p><strong>把“知道什么”这个问题，从模型能力中剥离出来，交给系统解决。</strong></p></blockquote><hr><h2 id="_12-3-从系统角度看-rag-在整体架构中的位置" tabindex="-1"><a class="header-anchor" href="#_12-3-从系统角度看-rag-在整体架构中的位置" aria-hidden="true">#</a> 12.3 从系统角度看：RAG 在整体架构中的位置</h2>',25),h=n('<figure><img src="'+o+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>从这里你可以看到：</p><ul><li><p>RAG 不是“总会发生”</p></li><li><p>它是一个<strong>条件触发的系统能力</strong></p></li><li><p>并且发生在：</p><blockquote><p><strong>上下文构建阶段，而不是 Prompt 阶段</strong></p></blockquote></li></ul><hr><h2 id="_12-4-一个常见误区-rag-万能增强器" tabindex="-1"><a class="header-anchor" href="#_12-4-一个常见误区-rag-万能增强器" aria-hidden="true">#</a> 12.4 一个常见误区：RAG ≠ 万能增强器</h2><p>在引入 RAG 后，很多团队会产生一个错误预期：</p><blockquote><p><em>“只要接了 RAG，模型就不会胡说了。”</em></p></blockquote><p>现实往往相反：</p><ul><li>检索结果不相关</li><li>模型曲解文档</li><li>回答看似“有依据”，但其实错误</li></ul><p>这引出一个非常重要的事实：</p><blockquote><p><strong>RAG 的失败，大多数并不是发生在“生成阶段”，而是发生在“检索之前”。</strong></p></blockquote><p>一旦你真正理解了 RAG 的系统定位，下一个、也是更残酷的问题就会出现：</p><blockquote><p><strong>为什么我已经“接了 RAG”，效果还是不好？</strong></p></blockquote><p>很多失败的 RAG 项目，会把问题归因于：</p><ul><li>向量模型不够好</li><li>大模型不够聪明</li></ul><p>但在绝大多数情况下，真正的瓶颈出现在一个<strong>更早、也更基础的地方</strong>： —— 文档是“如何被拆解与表达的” 。</p><p>就像用渔网捕鱼：如果网眼太大，小鱼会漏走；如果网眼太小，会被水草缠住。渔网的设计（对应 Chunk 设计），比 “用什么材质做渔网”（对应向量模型）更重要。</p><hr><h2 id="_12-5-文档-→-chunk-→-embedding-检索效果的真正上限" tabindex="-1"><a class="header-anchor" href="#_12-5-文档-→-chunk-→-embedding-检索效果的真正上限" aria-hidden="true">#</a> 12.5 文档 → Chunk → Embedding：检索效果的真正上限</h2><h3 id="_12-5-1-chunk-设计-被严重低估的工程决策" tabindex="-1"><a class="header-anchor" href="#_12-5-1-chunk-设计-被严重低估的工程决策" aria-hidden="true">#</a> 12.5.1 Chunk 设计：被严重低估的工程决策</h3><p>Chunk（文档片段）是 RAG 的 “原子单元”—— 系统检索的是 Chunk，注入上下文的也是 Chunk。Chunk 的大小与切分方式，直接决定了：</p><ul><li>能否被检索到：如果 Chunk 包含的信息与问题无关，再先进的检索器也找不到它；</li><li>被检索到后是否 “刚好有用”：如果 Chunk 太大，包含大量无关信息（噪声），模型会被干扰；如果太小，关键信息被切碎（比如一个流程的 “步骤 1” 和 “步骤 2” 被分到两个 Chunk），模型无法理解完整逻辑。</li></ul><p>在企业知识库助手中，Chunk 设计的常见问题：</p><ul><li><p>Chunk 太大：</p><ul><li>典型场景：直接把整篇文档作为一个 Chunk（比如一份 10 页的产品手册）。</li><li>问题：检索时容易 “误中”—— 比如文档里只有 1 段讲 “定价”，但因为整个文档被检索到，模型需要从 5000 字中找答案，很容易被其他内容干扰</li></ul></li><li><p>Chunk 太小：</p><ul><li>典型场景：按固定字数强制拆分（比如每 100 字切一段）。</li><li>问题：语义被切碎 —— 比如 “审批流程需要部门经理签字后提交给 HR”，如果被拆成 “审批流程需要部门经理签字” 和 “后提交给 HR” 两个 Chunk，单独检索到任何一个都无法理解完整流程。</li></ul></li></ul><blockquote><p><strong>Chunk 设计，本质上是在做信息密度的权衡：既要让每个 Chunk 包含 “足够完整的语义”，又要避免 “包含过多无关信息”。</strong></p></blockquote><hr><h3 id="_12-5-2-一个工程化的-chunk-决策思路" tabindex="-1"><a class="header-anchor" href="#_12-5-2-一个工程化的-chunk-决策思路" aria-hidden="true">#</a> 12.5.2 一个工程化的 Chunk 决策思路</h3><p>你可以从这样的问题开始思考：</p><ul><li>用户的问题，通常对应文档的哪一层？是段落级？章节级？还是流程级？</li><li>文档本身的结构是什么？是否有天然的语义边界（比如标题、列表、表格）？</li></ul><p>Chunk 应该尽量与 <strong>“被提问的最小语义单元”</strong> 对齐。举几个企业场景的例子：</p><ul><li><p>场景 1：FAQ 文档（比如 “IT 支持常见问题”）</p><ul><li>特点：每个问题对应一个独立答案（比如 “如何重置密码？”→ 步骤 1-3）。</li><li>最优 Chunk：按 “问题 + 答案” 成对切分（每个 Chunk 包含一个完整的问答）。</li><li>理由：用户的问题往往直接匹配 FAQ 中的问题，精准切分能确保检索到的 Chunk 刚好包含答案。</li></ul></li><li><p>场景 2：产品手册（比如 “CRM 系统操作指南”）</p><ul><li>特点：按功能模块划分章节（比如 “客户管理”→“新增客户”→“字段说明”）。</li><li>最优 Chunk：按 “功能子模块” 切分（比如 “新增客户的 5 个必填字段” 作为一个 Chunk）。</li><li>理由：用户的问题多是 “如何操作 XX 功能”，与子模块的语义边界高度匹配。</li></ul></li><li><p>场景 3：会议纪要（比如 “Q3 销售策略会记录”）</p><ul><li>特点：包含多个讨论点（比如 “目标调整”“资源分配”“风险应对”）。</li><li>最优 Chunk：按 “讨论主题” 切分（每个主题的讨论过程 + 结论作为一个 Chunk）。</li><li>理由：用户可能问 “Q3 的销售目标是多少”，需要定位到 “目标调整” 主题的 Chunk。</li></ul></li></ul><p>此外，Chunk 的元数据（Metadata）也很重要 —— 比如给每个 Chunk 标记 “文档类型”“更新时间”“所属部门”，能帮助检索器进一步筛选（比如优先检索 “2024 年更新” 的文档）。</p><hr><h2 id="_12-6-rag工程化建设" tabindex="-1"><a class="header-anchor" href="#_12-6-rag工程化建设" aria-hidden="true">#</a> 12.6 RAG工程化建设</h2><h3 id="_12-6-1-从-调用-rag-到-集成-rag-系统视角的转变" tabindex="-1"><a class="header-anchor" href="#_12-6-1-从-调用-rag-到-集成-rag-系统视角的转变" aria-hidden="true">#</a> 12.6.1 从“调用 RAG”到“集成 RAG”：系统视角的转变</h3><p>很多教程里的 RAG 看起来像这样：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>query → embedding → 向量检索 → 拼 prompt → 调模型
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>这在 Demo 中没问题，但在真实系统中，这种方式是不可持续的。</p><p>原因在于：</p><ul><li>RAG 不是一个<strong>函数</strong></li><li>它是一个<strong>长期存在的系统能力</strong></li></ul><p>当我们需要集成RAG能力时，必须完成一个视角转变：</p><blockquote><p><strong>RAG 不是“我什么时候用一下”，而是“系统在什么条件下，必须依赖它”。</strong></p></blockquote><h3 id="_12-6-2-rag-的第一个工程问题-何时触发" tabindex="-1"><a class="header-anchor" href="#_12-6-2-rag-的第一个工程问题-何时触发" aria-hidden="true">#</a> 12.6.2 RAG 的第一个工程问题：何时触发？</h3><p>一个成熟系统，<strong>不会对所有请求都使用 RAG</strong>。</p><p>典型触发条件包括：</p><ul><li><p>用户问题包含：</p><ul><li>企业私有名词</li><li>内部流程 / 制度</li></ul></li><li><p>问题涉及：</p><ul><li>时效性</li><li>明确版本</li></ul></li><li><p>模型置信度不足（如多次自我矛盾）</p></li></ul><figure><img src="`+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>',47),g=n(`<h3 id="_12-6-3-rag-的第二个工程问题-接口如何定义" tabindex="-1"><a class="header-anchor" href="#_12-6-3-rag-的第二个工程问题-接口如何定义" aria-hidden="true">#</a> 12.6.3 RAG 的第二个工程问题：接口如何定义？</h3><p>在进阶系统中，RAG 模块不应该“直接拼 Prompt”，而应该返回一个<strong>结构化结果</strong>：</p><div class="language-json line-numbers-mode" data-ext="json"><pre class="language-json"><code><span class="token punctuation">{</span>
  <span class="token property">&quot;documents&quot;</span><span class="token operator">:</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{</span>
      <span class="token property">&quot;id&quot;</span><span class="token operator">:</span> <span class="token string">&quot;doc-123&quot;</span><span class="token punctuation">,</span>
      <span class="token property">&quot;content&quot;</span><span class="token operator">:</span> <span class="token string">&quot;...&quot;</span><span class="token punctuation">,</span>
      <span class="token property">&quot;score&quot;</span><span class="token operator">:</span> <span class="token number">0.87</span><span class="token punctuation">,</span>
      <span class="token property">&quot;source&quot;</span><span class="token operator">:</span> <span class="token string">&quot;HR-policy-v3&quot;</span>
    <span class="token punctuation">}</span>
  <span class="token punctuation">]</span><span class="token punctuation">,</span>
  <span class="token property">&quot;query_intent&quot;</span><span class="token operator">:</span> <span class="token string">&quot;policy_explanation&quot;</span><span class="token punctuation">,</span>
  <span class="token property">&quot;confidence&quot;</span><span class="token operator">:</span> <span class="token string">&quot;high&quot;</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这样做的好处是：</p><ul><li>Prompt 构建不被 RAG 实现绑死</li><li>便于调试与评估</li><li>便于多模型 / 多策略切换</li></ul><blockquote><p><strong>RAG 的输出，是系统资产，不是 Prompt 片段。</strong></p></blockquote><h3 id="_12-6-4-rag-的第三个工程问题-模型如何-被迫使用-检索结果" tabindex="-1"><a class="header-anchor" href="#_12-6-4-rag-的第三个工程问题-模型如何-被迫使用-检索结果" aria-hidden="true">#</a> 12.6.4 RAG 的第三个工程问题：模型如何“被迫使用”检索结果？</h3><p>这是<strong>最容易被忽略、但最致命的一点</strong>。</p><p>即使你检索回了正确文档，模型仍然可能：</p><ul><li>忽略它</li><li>曲解它</li><li>混合自身知识胡说</li></ul><p>因此你必须在系统层面做约束。</p><h4 id="一种常见的工程策略" tabindex="-1"><a class="header-anchor" href="#一种常见的工程策略" aria-hidden="true">#</a> 一种常见的工程策略</h4><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>- 所有结论必须引用提供的文档
- 不允许基于“常识”补充
- 文档不足时，必须回答“不确定”
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="`+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>',14),k=n(`<blockquote><p><strong>RAG 的成功，50% 取决于“生成约束”，而不是“检索准确率”。</strong></p></blockquote><h3 id="_12-6-5-rag-的第四个工程问题-失败如何被定位" tabindex="-1"><a class="header-anchor" href="#_12-6-5-rag-的第四个工程问题-失败如何被定位" aria-hidden="true">#</a> 12.6.5 RAG 的第四个工程问题：失败如何被定位？</h3><p>在应用的实际表现中，一定会遇到这个问题：</p><blockquote><p><em>“现在效果不好，但我不知道是哪里出了问题。”</em></p></blockquote><p>一个可诊断的 RAG 系统，至少能回答：</p><ul><li>是不是根本没召回？</li><li>召回的内容是否相关？</li><li>模型是否使用了召回内容？</li><li>回答是否超出了引用范围？</li></ul><p>这要求你在系统中明确区分：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>检索失败 / 召回噪声 / 生成违约
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>否则，你永远只能“凭感觉调”。</p><h3 id="_12-6-6-企业知识库助手中的-rag-集成示例" tabindex="-1"><a class="header-anchor" href="#_12-6-6-企业知识库助手中的-rag-集成示例" aria-hidden="true">#</a> 12.6.6 企业知识库助手中的 RAG 集成示例</h3><p>最后以企业知识库为例，我们看一下RAG的集成示例图</p><figure><img src="`+u+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>',12),m=n('<h2 id="_12-7-本章小结-rag-是系统工程-而不是检索技巧" tabindex="-1"><a class="header-anchor" href="#_12-7-本章小结-rag-是系统工程-而不是检索技巧" aria-hidden="true">#</a> 12.7 本章小结：RAG 是系统工程，而不是检索技巧</h2><p>通过这一章，你应该已经形成一个完整认知：</p><ul><li>RAG 不是模型增强，也不是简单的“查资料”</li></ul><p>而是：</p><blockquote><p><strong>一种用于突破模型知识边界的系统补偿机制。</strong></p></blockquote><p>同时你也应该清楚地意识到：</p><ul><li>RAG 的效果上限，很早就被文档结构与数据工程决定了</li></ul><p>这意味着：</p><blockquote><p><strong>RAG 是一个数据工程与系统设计问题，远早于它是一个模型问题。</strong></p></blockquote><p>即便我们通过精心的 Chunk 设计为 RAG 打下了坚实基础，也无法完全避免检索失败的可能 —— 毕竟用户的问题可能天马行空，知识库的覆盖也总有边界。那么，当检索真的 “空手而归” 时，系统该如何应对才能避免陷入 “胡编乱造” 的陷阱？</p>',10);function b(v,R){const a=t("Mermaid");return l(),s("div",null,[c,i(a,{id:"mermaid-138",code:"eJyVVVFP21YYfd+vuFLVJ6CQpFmzaGq1Aq22tZXWdU/RHhL7JnikNnKcAVonhUKCAyHQJSW0QFIo0EwTySiMgJ3S/1LlXjtP7Cf0s41NAq7U4gew+c75zj33fPfGxPDYCHo09BWCn6tXEX3VIM1FUnvZOs6eNl/SXJbMVfTpd0TOaHvqaTNH1ENN3dX2VSKX2ukFPfvWhDLxcCIxhKOI48eS0gOBxSjKxePBK9gT9Udxb0IShVEcvDLgDQRYz9lr3zjHSiNB79hErygkeRazQc/A2EQ3YVyIcUwHYdSH/VG/Q/gN470RGfgSwt+SbAx3EkaB0maAV/8NZsCFkOXCjwWeDUpiEjvs/ovkjCB2cTP4OmYcbsbrCfgj3dy+C2J7owIv9Y1jLjYiBSNCnO1uICSlCwYHon4ccFr4AgHsszt+lh+8IHVJDhuPwxdhjcfmY8OJkbAohieDPuRzOANAeR6gI/J+mv43pVXnIUD6SYGkt9GHzF+IziyS1RMib9HlXfOD3qhrxb12Kkvn/zbh34W0YpXKjXap1t5c+TYi9t/8JYFF9FMSi5O/BoPB83D19d1Et0MWpbaU0d+8Rv2opapkbsPqYKK/5yXMS+ghZoQYz0mcwEPVo3BiFP0AIXgM/zPLTpuyXs8Q+Z/2WoruPYWgk7IhvqXMk1q5n8ir0KifNLZJuqH/Ow0iT5tZQ4+TTVO+bQHMCi3WLRnGDK3UydIOUOs7U2RruT1d1Srb0M8sv22uZPCPTxXdMgU+wJhFwxMSFvlwHP3IC+NxDCG+9SdocOLcpQG8JSczHqN96RXQdnKa5murBzS/rRUrVF4yQYOGkicf1nMIhIAjJJPW3tUshH64T7YqsOgnaCjUiTTVDXEiZiR0F/NYDBsm26aSitJS8ujevfuok40ohTP/zsPsIt4L4i8bYkXpdUo72Oyh5RmiKj2XF7GWRmAoKDDwG7Bt6db7DToFX2AFwyELTqsbZL1kan2IJZHDv4O39wU2Gcf2Ahy91Phj0y0C9sCb7YfN3bwTah3NtY7m6fKspdCkG4SxxhMSgt8wTEmm0yj6/IAsyZYsTS3Q8loPqR/r9XWSz5DFt65pu2P2uhsy3O3YDOP18k7o5QVo0OmjMZizyuV9uDDH+1UYLdgI7ekxXclbE9tSGyTVNAuNsyP0f6WQO6tvp1T95JmRuhcK3Bpn9c0Vklumz2UtP2vcIJ1RWFPoi7qVKFJYIEoRCpy4WptGS4f6mx2y+Mzyx9BrH1n2CF0DJ4xvjnqttku2DqzLDNSQpgIKrFiRnAIDahbGOX70Z2kyjtEAss+7rz3G031+eq7Z5/w5wuMg3C4MN4TXQbid0W4In4NwuzjcENcdhNu96Ibwf0aPj+A/tno="}),h,i(a,{id:"mermaid-485",code:"eJylk19P01AYxu/5FCch3DHdGItNYyDgDBcmJBq8Wrgo7dtRrS057QIoJkPc2OIcECcif4SBwK5WEQh/HOO7mJ3T7mp+BE/L1g1vjLHn6jTn/b3Pe57nyKo+LU4K2ERj0S7Evp4eRHfOSGWJlNdrF9lGZZ1+mbfLydpVmmTS5OiNd0pUBcOIgowUbSphjuoSIFlRVb4bQnJEhl7DxPpz4LuDfRwnhZrbwLQimZN839RML9YTmgQSHwqyjaxrZsBQXgIf6p+auc1/lpDi0MGXZTkMQZ8vR+6Jwda2gy8pwgtdk3gTJ8BvFvlLLz1h/jEMJ0eA85uFOQ7C4n8Mg4V45ygi9EMLx3eLfSEuMvFv9LZj5+R6gZ7O26V3vmPOmWUXjurJH051xTs4FLMLJZo5q38q13fX7k/guwNPDcDocQLw7DjP820zA4EBNPyKrllk+aBW3XJOV8nX1fpCyd7ed6z0oFc7CiChhzMmYE1Q0SNNn1aBmTX4mpF827zGLZksQLRgMV2k+rZRyTjFfZJKkc1L+tlyrBO6lm9UcmSjSncWSeWS5FYblaxXOuzqmfu5lUNMDisk6ZR9Vb7Rws7MoQcxe+OE5vftwjbNLHvqogoG0UQjoAEWTEXX3AHbDndyN1OITcq49c3kjabadZHOsz8MHY05hwdkaQU9GRrxwGNYicfZrbG9i2x66ltBlqx6sv1s6HHJOdxj7Nr5e3LxnWa/3fbqZkBNNyH2a/tDrumkW57LunK83nQvaZ/supdzcewUi/Rjxs4v1s7zZDFtF8s0eTjuUQxzVgWP1UqY4C4/YROSu1oJkwSDvXsszPJhFO4MVmfouGaE2U3dYZFw2V2/AeV02FQ="}),g,i(a,{id:"mermaid-554",code:"eJyVlFtPGkEUx9/7KSYxPilWoLR005h4SZqmtmmsb6QPCwyydWXJslZ9Q61cFCut16AIakVewFhQkd3KdzHM7PJEP0Jnr4LxoWWemD3n9/+fOWdmiqfDQTA59gSQX28vwPlrJG2gcqZ5k2xLGZxKotWcsvwbJWLyhdiWUki8ksWSXBFRYre1sq4kf2mpPpaORMZgADCh8KzwnvNDEGBYluqB9oArAPsjAs9NQ6pn0OF2++3GX9sc4xeClCM8389zsyE/9FP2wfB8N5DlphhfBzDghK6AywK+9DleeAf/B+jjeNjJ88Fn0GfxfA672+Xt5jkf8PoDXEiwzUFmKihQXo71P3DMznQegDvggm6L73S7odOU+ye/IU7o8kury+J5/eoyeX46EqR5nl6gnMBpMd0Eed/gGmos48tFubimNvgkKlePZXETHx6Au9gPINcLOJvDlSJaOdU2xsffAXkrhxNpDTHs6Ux55eWfDk1AgWfgF5oFEzAyywoRbbctJSa5sO0tkPdraKWCd+L46EROxnH5si0lP1EUdT8qNtsQGPE0a6vN2poaqKlrlFFy0nBeAG9Cn6FPYLiQycbbVZRO6Lw+nJfI1OJ8HMVjKLVj8K3J0YyPaCqjHiRty1tFvUzwgedmwoKuxAi0KgCIJDlNmgkJnZ+JpFJYxBdLRIX4w9lTXE/3yWSrdIx3883bLF4v6+dkyJtzpqmPaupjHnKaJk4PRrWC4SjzFW2cNxtHaP9QLm2ZNeiz9LB9laJydkLaJy/d4L1vejFN8RpFJS1QHRnPn9xmyohvRUXl9juJb0UzSiNu1GX0Gkl1lLjSC9KtkGtO8ChWARPDr43m30UP0I2onCXvotnWbrl1vEei8EYarccN2/WCaticVrPqAVK2ugesEuRyCf2s6g8NsaTKp3b0gVQah0r1HEfPtFiWCU1/FBZYCAaBOe7P7erqvj72AZdxZ+4z7FbGYw/EYxkOK+OxJ8ChZfwFAXVXew=="}),k,i(a,{id:"mermaid-612",code:"eJylVltvGkcUfs+vWCnKE3HKJdgUValiO3EiJb0k6RPqA7CzeBvCWsvS2G0q4dhg3DhA6mtsE2zHxkgVIAfHl4WQ/xIxs8uT+xN6ZhYWcLdu1bJIMMOc7/vOmXMhJPsnxrlHo5c4eF25wpGtY1zP4PJ683T+rL6Oa0daraRVazi1ilNJ7aDWSrzU59+d1RdwXcULK3hBhX2sFhhAMOyPRkeRwImRiZjylcQjThDDYe9l5BDcAroaVWTpMfJetjs9Ht7RXg48FXll3OucmLwqS7EIj3ivww4LQYooA1HxJ+R1uCYm+/HDUkgM9uALLuQW3Cb+50HnUMD+P/B/iPEh1IsvAEMHEJbuoaDdAp8X/U+kCO9V5Bgyydz/wBWU5D6qILqOgiZV0OnwuAP9VABh4cpTJIbGFW9ACvMX8skoGgv3Xo4gIHvAaTIiYdDSuX8dPCmmnLt9j+BGHpPA5fEgV8fD/0IQkZS+iPnpY8IHePp04Hl/dNwvy/4pr4tzmRSefgZgv9StgBP8cYa8n9aKL6AC9MYiTux9Sv5GZjN4o6FXkjj1Oyz144q2dIBTu2SlxExv+rSlIkkdt1bLrZ21LwLyZze+iyKZ+zaG5KnvvV5vtyYGBm5ww75eQHb8bkRBEYV7gIJSKCIqohRh22f1VGszTg6eGwVo0/cLOPOK5LabtSOD/6w+TwnMomB6hhnNyM9krYKzBcOIe3Bz7EsG+kgWQyFQRzd+AVsz4c04gIO4MeuAEJDVLRBAj3LgOKdtHJL0nraUJ6ksOzxCmZ59yi1wQARqcTKhfShr+T3wTT+q4t08CHzGjfp6LZmKUVFGQYUbQxEk+3sdxnm1qaa5e/fuc71oWF1s+9rNsXOCnSAY1OqFaVMwThSN62wtNuBQr+bNBAfxMSKMd1daM0WDiAm+5aMIpLiNc6tMGF3el/hYGHV0mrLI27h2uGPT5udI+b1W2mjFoX0ypZ3yZrS32KXc9mm1RfJmFvqnYceWmwz0IfSOoBKTEQ+JoMgi+tEfhm+0ZKMdVrI1R6pFktsjataGE9XWUrn5cZtMV0gmizMrbeJunTPq24x6zNc8+bV58oKszFEBNRW/LjLYEagGNKlwwzExzCPZpFo+xNmUoc+GK6d6JYfTSZx5Z4OhoNXymlogOXq/f03AMUZ4x2ccMW6d+0aWnkwoHUaoUb8IKd9NgN4DQA/XSBO/vkxry/BXg63SDuRks5EjL8sGrlWs7zD+uz6aRLSK51SGSpdfs+wxky1TgfDZ4IOo8M5q67N4441WWvr7XGs3iWpR338LCac9PyVraTPPmrVjHK+zs7RV+f7ILy60TVrxmt54RYvqtarPfDC6SLO+RlNhOaWl52CyQprZeoMGHDhZNVKNrB7BORLfb56k8WlN3583+g0dyIl6a7vRiudhGtNfmVfwnbrQ6ZidznANIkP3ONMjrVzCu4fG+KeDn013o6SMGQ+QbRe6VRQWI48fKlNhxNm5TvsddNCnv7s7rsEQPGfhMC2spqmVhdO0sJogVhYu08JqjFpZXDctrMaglYXbtLD612FlMXihKqeFxdDFFnD+T/R/Wjg="}),m])}const f=e(d,[["render",b],["__file","18.RAG.html.vue"]]);export{f as default};
