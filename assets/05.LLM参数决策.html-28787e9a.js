import{_ as e,V as o,W as p,$ as a,a1 as t,Y as n,F as l}from"./framework-094145d2.js";const i="/tutorial/imgs/column/llm/05-1.webp",u="/tutorial/imgs/column/llm/05-2.webp",c={},r=t('<p>在了解 LLM 的基本工作方式之后，我们来看一个<strong>实际体验中很常见的问题</strong>：</p><blockquote><p><strong>为什么同一个模型、同样的 Prompt， 有时表现很好，有时却很糟糕？</strong></p></blockquote><hr><h3 id="_2-1-一个常见困惑-为什么效果忽好忽坏" tabindex="-1"><a class="header-anchor" href="#_2-1-一个常见困惑-为什么效果忽好忽坏" aria-hidden="true">#</a> 2.1 一个常见困惑：为什么效果忽好忽坏？</h3><p>在实际使用中，我们可能都遇到过，选择了更牛的模型、也按照要求反复调整了提示词，在demo中表现挺好，可是只要一上线，各种问题就来了，如</p><ul><li>输出不稳定</li><li>有时啰嗦，有时过于简短</li><li>出现不符合现状或者自相矛盾的结果</li></ul><p>然后我们能干啥呢？ 换个模型？ 调整下提示词？ 或者随机改一下传参？ 即便是某一次碰巧搞定了，也不知道为啥。 将确定的编程过程整成了玄学，一个雷放在那儿、随时可能爆炸，这可以说是一个码农的噩梦了。</p><p>但问题到底出在什么地方呢？LLM开发让人头疼的一点也在于此，不像我们的业务代码，还可以通过debug，单步到核心逻辑去研究一下；大模型对于开发者而言，更多的是一个黑盒，想调试也束手无策</p><p>一般这种场景下，问题有可能并不在模型本身，而在于：</p><blockquote><p><strong>你是否意识到：参数本身就是“行为策略”？</strong></p></blockquote><p>当我们说 “参数是行为策略”，并不是指那些随意调整的 “玄学数值”。 比如 <code>temperature</code> 和 <code>top_p</code> 这两个最常用的参数，很多地方将他们描述为 “控制创造性的调味料”，这种描述挺贴切的，但是在具体的编码活动中，我们可能更希望有一个确切的数值标准，如果有不同的应用场景的最佳参数设置实践，那就更完美了。</p><p>接下来，我们仔细盘一下这两个参数 —— 它们到底在控制 LLM 的什么行为？</p><hr><h3 id="_2-2-temperature-top-p-不是-调味料" tabindex="-1"><a class="header-anchor" href="#_2-2-temperature-top-p-不是-调味料" aria-hidden="true">#</a> 2.2 temperature / top_p 不是“调味料”</h3><p>网上很多教程中，将<code>temperature</code> 解释为 <strong>“控制创造性，越大越发散”</strong> 的核心控制参数</p><p>这句话<strong>挺贴切的，从工程化的角度改怎么理解呢？</strong>。</p><p>使用更专业或者不那么容易懂语言进行解释</p><blockquote><p><strong>temperature 决定：模型是否允许偏离当前最优预测路径。</strong></p></blockquote><ul><li>temperature 越低 → 越倾向选择概率最高的 token</li><li>temperature 越高 → 越允许探索次优甚至低概率路径的 token</li></ul><p>从上面的描述，也可以得出一个直观的感受，temperature的取值，可影响</p><ul><li>输出是否可复现</li><li>行为是否稳定</li><li>是否适合被系统消费（而不仅是人阅读）</li></ul><hr><h4 id="那-top-p-又是什么" tabindex="-1"><a class="header-anchor" href="#那-top-p-又是什么" aria-hidden="true">#</a> 那 top_p 又是什么？</h4><p>如果说 temperature 是“整体发散程度”，那 <code>top_p</code> 更像是：</p><blockquote><p><strong>你允许模型在“多大的候选范围”里做选择。</strong></p></blockquote><ul><li><code>top_p = 0.9</code> → 只从累计概率前 90% 的 token 中选</li><li><code>top_p = 1.0</code> → 几乎不做限制</li></ul><p>请注意，所有的参数都不是独立生效的，他们必然是相互影响的（感觉像是废话，参数都是一起传给大模型的，肯定是一起工作的啊🤣）</p><p>在实际的开发过程中，不妨借鉴一下 “控制变量法” 的思路来进行调参：</p><ul><li><strong>固定其中一个</strong></li><li><strong>调另一个作为主策略</strong></li></ul><p>显然这又是一个痛苦和煎熬的反复拉扯过程~</p><hr><h3 id="_2-3-model-不是-越强越好-而是-是否匹配任务" tabindex="-1"><a class="header-anchor" href="#_2-3-model-不是-越强越好-而是-是否匹配任务" aria-hidden="true">#</a> 2.3 model：不是“越强越好”，而是“是否匹配任务”</h3><p>有一个反直觉的事实表现：</p><blockquote><p><strong>模型越大、越新，系统并不一定表现就越好。</strong></p></blockquote><p>在真实的应用抉择上，模型选择其实是一个多方面的<strong>工程权衡问题</strong>：</p><ul><li>能力上限</li><li>成本</li><li>延迟</li><li>行为一致性</li></ul><p>比如几个经典的应用场景下对模型的要求：</p><ul><li><p><strong>规则解释 / 企业知识问答</strong></p><ul><li>更需要稳定、克制</li><li>不一定需要最“聪明”的模型</li></ul></li><li><p><strong>创意生成 / 头脑风暴</strong></p><ul><li>可以接受不确定性</li><li>模型探索能力更重要</li></ul></li></ul><p>因此，一个更健康的视角是：</p><blockquote><p><strong>model 是能力边界，参数决定你是否触碰这个边界。</strong></p></blockquote><hr><h3 id="_2-4-max-tokens-你允许系统-说到什么程度" tabindex="-1"><a class="header-anchor" href="#_2-4-max-tokens-你允许系统-说到什么程度" aria-hidden="true">#</a> 2.4 max_tokens：你允许系统“说到什么程度”</h3><p><code>max_tokens</code> 经常被当作一个简单的“长度限制”。</p><p>但在系统层面，它真正控制的是：</p><blockquote><p><strong>模型是否被允许“继续展开思路”。</strong></p></blockquote><p>这在以下场景尤为关键：</p><ul><li>多步推理</li><li>解释性回答</li><li>Agent 场景中的中间推理</li></ul><p>如果 <code>max_tokens</code> 过低，导致 <code>推理会被强行截断</code> + <code>输出容易“看似合理但不完整”</code></p><p>如果过高，也会导致 <code>成本上升</code> + <code>模型更容易开始“自由发挥”</code></p><p>所以它本质上是：</p><blockquote><p><strong>对“思考深度”的一种工程约束。</strong></p></blockquote><p>看到这里不知道你脑海里是否和我有相同的感觉，这种大模型调用的传参，感觉变成了一个<code>经验学科</code>了，只有实际体验得多了，才知道什么场景、选择什么样的传参🤣</p><hr><h3 id="_2-5-stream-不是体验优化-而是系统架构选择" tabindex="-1"><a class="header-anchor" href="#_2-5-stream-不是体验优化-而是系统架构选择" aria-hidden="true">#</a> 2.5 stream：不是体验优化，而是系统架构选择</h3><p><code>stream</code> 可以简单的理解为 <code>能不能一边生成一边显示结果</code></p><p>在复杂系统中，它还包含着一些潜在的含义：</p><ul><li>是否允许<strong>增量消费输出</strong></li><li>是否能在生成过程中： <ul><li>中断</li><li>校验</li><li>触发后续逻辑</li></ul></li></ul><p>在 Agent / Tool / 长文本场景中：</p><ul><li>非流式 → 一次性黑箱结果</li><li>流式 → 可观测、可干预</li></ul><p>从上面的描述也可以看出，流式调用除了体验的优化之外，还有一些系统约束层面的能力</p><blockquote><p><strong>LLM可以是“一次性函数调用”之外，作为系统的一部分存在</strong></p></blockquote><h3 id="_2-6-用「任务类型」来决定参数-而不是凭感觉" tabindex="-1"><a class="header-anchor" href="#_2-6-用「任务类型」来决定参数-而不是凭感觉" aria-hidden="true">#</a> 2.6 用「任务类型」来决定参数，而不是凭感觉</h3><p>上面说了这些参数，那么在我的应用场景中，具体应该怎么设置呢， 全部用默认参数吗？</p><p>显然一个更合理的做法是：</p><blockquote><p><strong>先判断你在做什么任务，再决定参数策略。</strong></p></blockquote><p>比如根据实际场景，我是否需要一个发散性的回复，例如</p><ul><li>代码生成： <ul><li>temperature 低</li><li>输出结构稳定</li></ul></li><li>创意写作： <ul><li>temperature 高</li><li>接受一定不确定性</li></ul></li></ul><figure><img src="'+i+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>',68),d=n("p",null,"一组参数配合调参，应对不同的应用场景：",-1),k=n("figure",null,[n("img",{src:u,alt:"",tabindex:"0",loading:"lazy"}),n("figcaption")],-1),m=t(`<hr><h3 id="_2-7-把参数当成策略的一部分-伪代码示例" tabindex="-1"><a class="header-anchor" href="#_2-7-把参数当成策略的一部分-伪代码示例" aria-hidden="true">#</a> 2.7 把参数当成策略的一部分（伪代码示例）</h3><p>明确了 “任务类型决定参数策略” 的逻辑后，接下来我们再来看一下，代码实现上，可以怎么进行表现（好像也是简单的if/else 😊）</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">build_llm_config</span><span class="token punctuation">(</span>task_type<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> task_type <span class="token operator">==</span> <span class="token string">&quot;fact_query&quot;</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;model&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;stable-model&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;temperature&quot;</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span>
            <span class="token string">&quot;top_p&quot;</span><span class="token punctuation">:</span> <span class="token number">0.9</span><span class="token punctuation">,</span>
            <span class="token string">&quot;max_tokens&quot;</span><span class="token punctuation">:</span> <span class="token number">500</span><span class="token punctuation">,</span>
            <span class="token string">&quot;stream&quot;</span><span class="token punctuation">:</span> <span class="token boolean">False</span>
        <span class="token punctuation">}</span>

    <span class="token keyword">if</span> task_type <span class="token operator">==</span> <span class="token string">&quot;code_generation&quot;</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;model&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;code-model&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;temperature&quot;</span><span class="token punctuation">:</span> <span class="token number">0.2</span><span class="token punctuation">,</span>
            <span class="token string">&quot;top_p&quot;</span><span class="token punctuation">:</span> <span class="token number">0.95</span><span class="token punctuation">,</span>
            <span class="token string">&quot;max_tokens&quot;</span><span class="token punctuation">:</span> <span class="token number">1500</span><span class="token punctuation">,</span>
            <span class="token string">&quot;stream&quot;</span><span class="token punctuation">:</span> <span class="token boolean">False</span>
        <span class="token punctuation">}</span>

    <span class="token keyword">if</span> task_type <span class="token operator">==</span> <span class="token string">&quot;agent_reasoning&quot;</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;model&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;reasoning-model&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;temperature&quot;</span><span class="token punctuation">:</span> <span class="token number">0.3</span><span class="token punctuation">,</span>
            <span class="token string">&quot;top_p&quot;</span><span class="token punctuation">:</span> <span class="token number">0.9</span><span class="token punctuation">,</span>
            <span class="token string">&quot;max_tokens&quot;</span><span class="token punctuation">:</span> <span class="token number">3000</span><span class="token punctuation">,</span>
            <span class="token string">&quot;stream&quot;</span><span class="token punctuation">:</span> <span class="token boolean">True</span>
        <span class="token punctuation">}</span>

    <span class="token keyword">if</span> task_type <span class="token operator">==</span> <span class="token string">&quot;creative&quot;</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;model&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;creative-model&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;temperature&quot;</span><span class="token punctuation">:</span> <span class="token number">0.9</span><span class="token punctuation">,</span>
            <span class="token string">&quot;top_p&quot;</span><span class="token punctuation">:</span> <span class="token number">1.0</span><span class="token punctuation">,</span>
            <span class="token string">&quot;max_tokens&quot;</span><span class="token punctuation">:</span> <span class="token number">800</span><span class="token punctuation">,</span>
            <span class="token string">&quot;stream&quot;</span><span class="token punctuation">:</span> <span class="token boolean">True</span>
        <span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><hr><h3 id="_2-8-本章小结" tabindex="-1"><a class="header-anchor" href="#_2-8-本章小结" aria-hidden="true">#</a> 2.8 本章小结</h3><p>这一篇的内容主要介绍了大模型访问的几个关键传参，并且声明了大模型表现与参数配置之间的关系</p><ul><li>模型决定能力上限</li><li>参数决定行为方式</li><li>行为决定系统是否可靠</li></ul><p>在后面的章节中我们也会逐渐发现</p><ul><li>Prompt 是<strong>约束</strong></li><li>参数是<strong>策略</strong></li><li>RAG / Tool / Memory 是<strong>补偿机制</strong></li></ul><p>所有的这一切，都是围绕着 <strong>如何让一个不可靠的模型，在系统中变得可控。</strong> 努力</p><p>既然参数这一块已经说明，接下来自然就是重头戏，如何与LLM进行对话交流，所以下一部分，我们将正式进入 <code>Prompt</code> 工程，但不从“怎么写”开始，而是先回答：</p><p><strong>Prompt 为什么会失败？</strong></p>`,13);function g(b,q){const s=l("Mermaid");return o(),p("div",null,[r,a(s,{id:"mermaid-401",code:"eJyVlN9P21YUx9/7V1yp6lPLAoky0mjq1JK2e+qmdm/RHhz7mnoNdubYLdWYlECTJhAIKEBLCISuQJk2Etg6AfkB/0vle20/ZX/Cjq9r12V0UmMpku1zP+ec7/kei2nlCf+QUzX0feISgt+VK4huHZFelbTqxkl50KvTStmanTanT8hs05rpk1LRPOyyWD7NZbMJLCKNyz66pwgYiVI6Hb+MI2JYFK5lNVV5hOOXR66PfimE398OPZEE7WE8nJm8piq6LGAhPjKcmfyY96MujOMAUBTFCB72gWJ0lB/2bgNAQeImFFmIa6qOfXr0PDzDqdxEEB7BUTHqw6/z4dHUBfBPV8vxmqTIwf5jYhTHfGIkFsMR/nOIsqJ91D7nXD4vJTiXxxO4LMxP5Z7GIyjiM2OA/DDPY3I2Q//Om3tz7NnNJOnXSHne6HbJ7KuvUmroxpiuqlgGF8Akf4jH4/5Eh4ZuoFs/05dtsrhLCnmrdUyqS3Tl9aDXZAdvpsFBKCE9xuo4lnn89S9w2p8fS+fVAc6hy20717VOl8BXpDpNVw7sXJnO/cZCbjnJpt5tVBDkGvRK5p+n5HkxZBzvWAd7g155Co0lNTyRwSqn6SpG74o1VoLRX7DrVdro0Nwbp3Z/wEFqo4CgCaCS0jp9Vg2R7Tr4nFET56hLjGr//vJ/qH5P+4uktE1X91lDbbI9by4cwAn2foypd/u9etZunh5O+0GegPfxT7oEee/jjKoIOi+lpLSkPb1Ax9vBTqbQnSRZ78CWujqiq8js1ujmM1JZtU5r5HmH0e9Ik1hADzD8XUUPYDF4p0kBfatrGV1z+vrg3mASdwhT6G6SrrXsXN2dhbncpKVFBr6nqBNcGn2nYl7KYnQXy46AQLqA6YkVlNQ1g7n+li7suPXCbOiLLbuRs3/dINurZmeXbjRhQOx0gkn5TdKdnmvBQDVjKobkj7FvRO3TFXnVWK92SKFA/9qz3oCb66TXAenM5o7VLsLHjkU5eziS/KdZqyAwGXJsggiUvwZGqptnXau9FDK6r82tfMjozJHWpv2iZe4vOwm9FfY5YZcDGpzjkOKa0W+EUErlJDmrga6SPB6yyh3aaEO78O4/PH+zWfFGf97st6yDGbry1lmb5T3rbBNkNf/YdR3iBgx6FTt/Rgrz9OjIODn0lAWTfgHSslY9qb0H4Uv/Aqr7u5s="}),d,k,a(s,{id:"mermaid-408",code:"eJyVlF1v2lYYx+/7KY5UdTcr4U0sBE2d1qYvN+ukLndoqgw+bll5mzFqqmUTpAFMQoA0IU2AJNAkLdIWSBvWEiDwXTqfY/uKfYQd29gxK70oiItjfH7P//n/n3OYYOSp/zHFcmBh/gogn2vXAK59QP0CapaF8+yoXxaGeziXFDp5lF1H3QKqDKR6Qz7M41pf6CSkTHvUz4n1JnlNPOsh/iVeOVBJ/iAVi81DBnBU7Mn9CA0BEwgGPVehk3Ew9PUYx0aeQM9V+9zsN7RjvLQ8DdDcY48junidjcTDNKQ9dlt0cZL3S5x+BE1AhmGc0GYAGdes36YvTUA6QIUiYdrDsXFo0F3/h0cplgqZ4U7oYlwGfM7vmPVNgX9eLeXnApGwuX8344Jug+h0u6HT/yXEcISbaJ9SvgbPRytfnUdTMZIuSz3zOIHTYLoJ8jLtDho+x38nxcaaknavh1br4GP6BRAbZ2QIcOIN4o/w9om64Xsvutgkg6C9Jr7tof21b32s9cYCCRksPIvCnz0ej5G4xXID3PwN77RQ8bX0OonfLmtQabCJMt1R/0Dd+wD+Gg+wEPzEUb4gBD/GuWic++53AjKiVovrkhGfxlstO1FrZo76vPhugDJpKyq00NG6mD9F1S7ebY36WXXnTUXO0sdqChBBS+CWV7jIAw6GopCluDip/zVAlS4hAS4SfRhVpRGo/OeO4QQZdbJJLhcwISvLrNKuMTNqmVtq1/PjruVqgohER2V8cozzDbGY1ru+DyENfogHuYAlxsEoeACpWCQcCD+a0vn8pPTbXlwoEl0gRC0+5EjO4ZihNrEsp9bl0hBvZ3D1L72kqvNyFM3QvRwgOpfAHa+8W0T8+ylQ4qvYTOih5bTiuMSL+cx09GRSDpIUSiWlZgfnX4ntV4SI+ApeKVgVX2oflBHTk7IAqX6MUg050ZMGG+bcdJ13vWaWOqmKFeYgrQCtNnAi+UmQRj3SBep3iQo5UUOrlalB3lWDvOfFuayS4ulzXGpPMUdrBaV3hYuqFfhYKhCOcRE2RJIEZBKl5gBX2qT2p07pLqkdp/BZQ3pzqFhVWMal8exKrTbeyasvKsfe7v33YDM3Ppm6Z2Whu4aa+/LLpniy9U8iKfQOxVpS3DrAfJEsxWFPam0opfWLw8A5NJzmpIHTWiE7x50VNnDpkCzx9jmu83h/BSf2puKcBPeirpmuyOpMHi8yl6jIK/OTJ1fKe22cTInwthn7H7aZubFRBt64qVSDpOE+zh+Tt6XdstgdCp11dP4OZ0+FTk+/xLTjTs7hDMlPtU3PU3/gUB/cBl+BO+R37/IP55X/AGwiXbk="}),m])}const h=e(c,[["render",g],["__file","05.LLM参数决策.html.vue"]]);export{h as default};
