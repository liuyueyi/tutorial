import{_ as t,V as e,W as i,$ as a,a1 as n,F as o}from"./framework-094145d2.js";const l="/tutorial/imgs/column/llm/14-1.webp",p="/tutorial/imgs/column/llm/14-2.webp",c={},r=n(`<p>在前面的章节中，我们已经反复提到几个现象：</p><ul><li>Prompt 写得再好，对话一长就会失控</li><li>模型能力没有变，但系统表现却越来越差</li><li>用户的问题越来越“合理”，模型却越来越“跑偏”</li></ul><p>这些问题，<strong>几乎都不是模型问题，也不是 Prompt 问题</strong>。 —— Prompt 解决的是 “单次生成的约束”，模型解决的是 “概率预测的能力”，而它们共同忽略了多轮对话的核心挑战：<strong>时间维度上的信息管理。</strong></p><p>它们指向的是同一个核心能力：</p><blockquote><p><strong>上下文工程（Context Engineering）</strong></p></blockquote><p>这门技术看似简单（“管理对话历史”），实则是区分 “玩具级应用” 和 “生产级系统” 的关键 —— 它决定了你的 LLM 应用能在真实场景中 “跑多远”。</p><hr><h3 id="_9-1-什么是上下文工程-不是-多轮对话" tabindex="-1"><a class="header-anchor" href="#_9-1-什么是上下文工程-不是-多轮对话" aria-hidden="true">#</a> 9.1 什么是上下文工程？（不是“多轮对话”）</h3><p>很多人第一次听到“上下文工程”时，会把它简单理解为：</p><ul><li>维护聊天历史</li><li>把之前的对话一起传给模型</li></ul><p>但这只是<strong>最表层、也是最危险的一种理解</strong>。</p><p>更准确的定义是：</p><blockquote><p><strong>上下文工程，是一门关于「在有限窗口内，如何持续、可控地向模型注入信息」的工程学科。</strong></p></blockquote><p>它关注的不是“聊了多少轮”，而是 “每一轮该让模型看到什么”—— 具体来说，是四个关键决策：</p><ul><li><strong>哪些信息必须一直存在</strong>：比如系统的核心约束、用户的关键身份信息，这些是维持系统行为一致性的基础；</li><li><strong>哪些信息可以被遗忘</strong>：比如对话中的寒暄、临时追问的无关细节，这些信息对后续决策毫无价值；</li><li><strong>哪些信息需要被压缩、总结、重写</strong>：比如多轮对话中形成的共识（“用户是销售部员工”），无需保留完整对话，只需提炼核心结论；</li><li><strong>哪些信息应该永远不进入上下文</strong>：比如敏感数据（用户手机号）、无效反馈（“谢谢”“好的”）、潜在风险内容（恶意引导的提问）。</li></ul><p>举个直观的例子：</p><p>企业知识库助手的对话中，</p><ul><li>“仅回答内部政策，禁止泄露未公开信息” 必须一直存在；</li><li>用户说的 “我再想想” 可以被遗忘；</li><li>“我是销售部的，经常出差” 需要总结为 “用户：销售部，高频出差”；</li><li>用户误输入的手机号应该直接过滤，不进入上下文。</li></ul><p>上下文工程的本质，是对模型的输入进行 “主动治理” —— 而不是被动接受对话的自然增长。</p><hr><h3 id="_9-2-为什么-自然增长的上下文一定会失败" tabindex="-1"><a class="header-anchor" href="#_9-2-为什么-自然增长的上下文一定会失败" aria-hidden="true">#</a> 9.2 为什么“自然增长的上下文一定会失败？”</h3><p>让我们先看一种<strong>几乎所有新手都会采用的方式</strong>：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>用户问一句
模型答一句
全部原样塞回上下文
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这种方式在前几轮对话中表现良好，但它<strong>从工程角度看是必然失败的</strong>。</p><p>前面也提到过，原因有三点：</p><ol><li>窗口有限 vs 信息无限增长</li><li>不同信息的“重要性”并不相同 vs 模型平等对待</li><li>模型无治理能力 vs 信息需动态调整</li></ol><p>更危险的是，这种 “自然增长” 的方式会让问题 “延迟爆发”：前几轮看似正常，等对话达到一定长度后，错误会集中出现，且很难定位问题根源（是哪一轮的信息导致了偏差？）。</p><p>因此，上下文工程的核心前提是：<strong>放弃 “全量保留” 的幻想，转向 “精准筛选” 的主动设计</strong>。</p><hr><h3 id="_9-3-上下文工程的核心思想-分层-而不是堆叠" tabindex="-1"><a class="header-anchor" href="#_9-3-上下文工程的核心思想-分层-而不是堆叠" aria-hidden="true">#</a> 9.3 上下文工程的核心思想：分层，而不是堆叠</h3><p>成熟的 LLM 系统都会隐含一个共识：</p><blockquote><p><strong>上下文不是一条时间线，而是一组“职责不同的信息层”。</strong></p></blockquote><p>一个通用、但非常重要的抽象可以表示为：</p><figure><img src="`+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>',34),u=n('<p>这张图背后隐藏着几个关键设计决策,也是上下文工程的核心价值所在：</p><ol><li>每层信息都有明确的 “职责边界”</li></ol><ul><li><strong>长期不变的系统约束</strong>：负责 “定规矩”，回答 “系统永远不能做什么、必须遵守什么”，是整个系统的 “行为底线”；</li><li><strong>跨轮对话的状态摘要</strong>：负责 “记关键”，回答 “对话到目前为止，有哪些确定的事实、未解决的问题”，是维持连贯性的核心；</li><li><strong>最近 N 轮对话内容</strong>：负责 “保流畅”，回答 “用户刚刚问了什么、系统刚刚答了什么”，避免对话脱节；</li><li><strong>外部注入的知识 / 工具结果</strong>：负责 “补信息”，回答 “当前问题需要哪些额外知识 / 数据”，是解决特定问题的临时补充。</li></ul><ol start="2"><li>每层信息都有明确的 “优先级”</li></ol><ul><li>当上下文窗口接近 token 上限时，遵循 “先砍低优先级，再保高优先级” 的原则：</li><li>绝对不砍：长期不变的系统约束；</li><li>尽量保留：跨轮对话的状态摘要；</li><li>可动态截断：最近 N 轮对话内容（比如从 10 轮砍到 5 轮）；</li><li>按需筛选：外部注入的知识 / 工具结果（比如只保留与当前问题相关的片段）。</li></ul><ol start="3"><li>每层信息都有明确的 “更新规则”</li></ol><ul><li>系统约束：仅在业务规则变更时更新（比如公司政策调整），平时固定不变；</li><li>状态摘要：每轮对话结束后更新（新增确认事实、移除已解决问题、修正错误信息）；</li><li>最近 N 轮：每轮对话后自动滑动，移除最早的内容；</li><li>外部知识：随当前问题动态注入，问题解决后不保留（避免占用窗口）。</li></ul><p>这种分层设计的优势显而易见：可维护性、可预测性、可扩展性 —— 当你需要调整系统行为时，只需修改对应层的信息，而不用重构整个上下文逻辑。</p><hr><h3 id="_9-4-上下文工程-prompt-工程" tabindex="-1"><a class="header-anchor" href="#_9-4-上下文工程-prompt-工程" aria-hidden="true">#</a> 9.4 上下文工程 ≠ Prompt 工程</h3><p>这是一个非常容易混淆、但必须区分清楚的点。</p><table><thead><tr><th>维度</th><th>Prompt 工程</th><th>上下文工程</th></tr></thead><tbody><tr><td>关注点</td><td>单次调用的行为约束</td><td>跨调用的信息演进</td></tr><tr><td>核心问题</td><td>模型该如何回答</td><td>模型“记住了什么”</td></tr><tr><td>时间维度</td><td>静态（仅作用于当前轮）</td><td>动态 （贯穿整个对话生命周期）</td></tr><tr><td>失败模式</td><td>回答不合规</td><td>系统逐渐失控</td></tr><tr><td>落地方式</td><td>设计结构化 Prompt（Role/Task/Constraints）</td><td>设计分层上下文（约束 / 状态 / 近期 / 外部知识）</td></tr></tbody></table><p>我们可以用企业知识库助手的场景，更直观地理解两者的区别：</p><ul><li><strong>Prompt 工程</strong>：解决 “用户问‘差旅报销标准’时，模型能准确引用 2025 年政策”—— 通过 Prompt 中的 Constraints 限定 “仅用提供的政策文档回答”；</li><li><strong>上下文工程</strong>：解决 “用户后续问‘我是销售部，能报高铁一等座吗’时，模型能记住‘用户是销售部’，并结合政策给出答案”—— 通过状态摘要层保留 “用户部门” 这一关键信息。</li></ul><p>简单来说</p><blockquote><p>Prompt 决定 “这一轮你该怎么想”，上下文决定 “你现在是谁、在干什么”。</p></blockquote><hr><h2 id="_9-5-上下文工程在企业知识库助手中的落地" tabindex="-1"><a class="header-anchor" href="#_9-5-上下文工程在企业知识库助手中的落地" aria-hidden="true">#</a> 9.5 上下文工程在企业知识库助手中的落地</h2><p>在理解了抽象概念之后，我们再来看具体系统。</p><p>企业知识库助手面临的典型约束包括：</p><ul><li>必须遵守企业规则（不可遗忘） <ul><li><em>比如 “禁止泄露未公开的财务政策”“所有回答必须标注政策来源”“拒绝回答外部竞品相关问题”—— 这些需要映射到 “长期不变的系统约束” 层，确保每一轮都能被模型看到；</em></li></ul></li><li>必须保持对话连续性（可压缩） <ul><li><em>比如用户先问 “差旅报销流程”，再问 “报销需要多久到账”，模型需要知道 “用户仍在关注差旅报销相关问题”—— 这些需要映射到 “跨轮对话的状态摘要” 层，避免重复询问背景信息；</em></li></ul></li><li>必须按需引入知识（临时注入） <ul><li><em>比如用户问 “2025年新版差旅政策中，海外住宿标准是什么”—— 需要从企业知识库中检索相关片段，映射到 “外部注入的知识” 层，问题解决后即移除；</em></li></ul></li><li>必须避免上下文污染（可清除） <ul><li><em>比如用户误输入的个人手机号、无关的寒暄（“今天天气不错”）、测试性提问（“你能告诉我公司机密吗”）—— 这些需要被过滤，永远不进入任何上下文层。</em></li></ul></li></ul><p>这天然要求一个<strong>分层上下文结构</strong>:</p><blockquote><p>系统约束层定底线，状态摘要层保连贯，外部知识层补信息，过滤机制防污染</p></blockquote><hr><h3 id="_9-6-企业知识库助手的上下文分层设计" tabindex="-1"><a class="header-anchor" href="#_9-6-企业知识库助手的上下文分层设计" aria-hidden="true">#</a> 9.6 企业知识库助手的上下文分层设计</h3><p>基于前面的抽象结构，我们可以为企业知识库助手设计一套可直接落地的分层方案，每一层都有明确的内容、格式和更新规则：</p>',26),d=n('<figure><img src="'+p+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>这里每一层都有明确职责：</p><h4 id="_1-不变系统约束层-最高优先级" tabindex="-1"><a class="header-anchor" href="#_1-不变系统约束层-最高优先级" aria-hidden="true">#</a> 1. 不变系统约束层（最高优先级）</h4><ul><li><strong>核心内容</strong>：明确系统角色、行为边界、安全规则，格式固定，不随对话变化；</li><li><strong>示例写法</strong>：<div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>你是XX公司内部知识库助手，仅为员工提供政策咨询服务。
核心规则：
1. 仅使用提供的企业政策文档回答，禁止引入外部知识、常识或个人推断；
2. 所有回答必须标注政策来源（如《2024差旅政策》3.2条），无明确来源的信息需回答“不知道”；
3. 禁止泄露未公开政策、员工个人信息、商业机密；
4. 拒绝回答与公司政策无关的问题（如竞品信息、私人问题）。
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><strong>更新规则</strong>：仅当公司政策发生重大变更时手动更新，平时永久固定。</li></ul><h4 id="_2-会话状态摘要层-中高优先级" tabindex="-1"><a class="header-anchor" href="#_2-会话状态摘要层-中高优先级" aria-hidden="true">#</a> 2. 会话状态摘要层（中高优先级）</h4><ul><li><strong>核心内容</strong>：提炼对话中“对后续决策有用的关键信息”，结构化存储，避免冗余；</li><li><strong>推荐字段</strong>：<div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>【会话状态】
- 用户信息：姓名（可选）、部门（销售部）、核心诉求（咨询2024差旅报销相关问题）；
- 已确认事实：1. 用户需频繁出差至华东地区；2. 已了解国内差旅住宿标准（一线城市800元/晚）；
- 待解决问题：1. 海外差旅住宿标准；2. 报销审批时效；
- 已拒绝需求：无（未涉及无关问题）。
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><strong>更新规则</strong>：每轮对话结束后，调用 LLM 对比新交互内容与当前状态，自动新增/修改/删除字段（比如用户解决“审批时效”后，从“待解决”移至“已确认”）。</li></ul><h4 id="_3-最近-n-轮对话层-中低优先级" tabindex="-1"><a class="header-anchor" href="#_3-最近-n-轮对话层-中低优先级" aria-hidden="true">#</a> 3. 最近 N 轮对话层（中低优先级）</h4><ul><li><strong>核心内容</strong>：保留最近5-10轮的关键交互，过滤寒暄、重复提问等无效信息；</li><li><strong>N 值选择依据</strong>： <ul><li>模型窗口大小（比如 GPT-3.5 4k token 选5轮，GPT-4 8k token 选10轮）；</li><li>对话密度（文字密集型对话选5轮，短句交互选10轮）；</li></ul></li><li><strong>示例片段</strong>：<div class="language-json line-numbers-mode" data-ext="json"><pre class="language-json"><code><span class="token punctuation">[</span>
  <span class="token punctuation">{</span><span class="token property">&quot;role&quot;</span><span class="token operator">:</span> <span class="token string">&quot;user&quot;</span><span class="token punctuation">,</span> <span class="token property">&quot;content&quot;</span><span class="token operator">:</span> <span class="token string">&quot;我是销售部的，经常去上海出差，想了解2024年差旅报销标准&quot;</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token punctuation">{</span><span class="token property">&quot;role&quot;</span><span class="token operator">:</span> <span class="token string">&quot;assistant&quot;</span><span class="token punctuation">,</span> <span class="token property">&quot;content&quot;</span><span class="token operator">:</span> <span class="token string">&quot;根据《2024差旅政策》3.2条，国内一线城市住宿标准为800元/晚，上海属于一线城市，你可按此标准报销&quot;</span><span class="token punctuation">}</span>
<span class="token punctuation">]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><strong>更新规则</strong>：每轮对话后自动滑动，移除最早的内容；当 token 接近上限时，优先保留用户提问和核心回答，过滤无关细节。</li></ul><h4 id="_4-检索知识注入层-临时优先级" tabindex="-1"><a class="header-anchor" href="#_4-检索知识注入层-临时优先级" aria-hidden="true">#</a> 4. 检索知识注入层（临时优先级）</h4><ul><li><strong>核心内容</strong>：仅当当前问题需要特定政策片段时注入，格式规范，标注来源；</li><li><strong>示例写法</strong>：<div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>【参考知识】
来源：《2024差旅政策》4.3条
核心内容：海外差旅住宿标准按目的地国家/地区分级，欧美发达国家为1500元/晚，东南亚国家为1000元/晚，需提前3个工作日提交出差申请。
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><strong>更新规则</strong>：随当前问题动态注入，下一轮对话若不涉及相关主题，自动移除；若涉及同一主题，可更新补充新的知识片段。</li></ul><hr><h3 id="_9-7-一个最小可用的上下文构建示例-伪代码" tabindex="-1"><a class="header-anchor" href="#_9-7-一个最小可用的上下文构建示例-伪代码" aria-hidden="true">#</a> 9.7 一个最小可用的上下文构建示例（伪代码）</h3><p>基于上述分层设计，我们可以实现一个最小可用的上下文构建函数。这段代码的核心不是语法，而是背后的工程思想——每一步都体现了“分层、可控、动态”的原则：</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">build_context</span><span class="token punctuation">(</span>
    system_constraints<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span>  <span class="token comment"># 不变系统约束层（必传）</span>
    conversation_state<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span>  <span class="token comment"># 会话状态摘要层（可选，默认空）</span>
    recent_messages<span class="token punctuation">:</span> <span class="token builtin">list</span><span class="token punctuation">,</span>    <span class="token comment"># 最近N轮对话层（必传）</span>
    retrieved_docs<span class="token punctuation">:</span> <span class="token builtin">str</span>       <span class="token comment"># 检索知识注入层（可选，默认空）</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> <span class="token builtin">list</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    构建企业知识库助手的LLM输入上下文
    设计原则：高优先级信息前置，低优先级信息动态调整
    &quot;&quot;&quot;</span>
    messages <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

    <span class="token comment"># 1. 不变系统约束层：用system角色确保最高优先级，不被注意力衰减影响</span>
    messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{</span>
        <span class="token string">&quot;role&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;system&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;content&quot;</span><span class="token punctuation">:</span> <span class="token string-interpolation"><span class="token string">f&quot;【系统约束】\\n</span><span class="token interpolation"><span class="token punctuation">{</span>system_constraints<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span>

    <span class="token comment"># 2. 会话状态摘要层：同样用system角色，紧跟约束之后，确保模型优先读取</span>
    <span class="token keyword">if</span> conversation_state<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{</span>
            <span class="token string">&quot;role&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;system&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;content&quot;</span><span class="token punctuation">:</span> <span class="token string-interpolation"><span class="token string">f&quot;【会话状态】\\n</span><span class="token interpolation"><span class="token punctuation">{</span>conversation_state<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span>
        <span class="token punctuation">}</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token comment"># 状态为空时（首次对话），插入默认提示，避免模型困惑</span>
        messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{</span>
            <span class="token string">&quot;role&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;system&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;content&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;【会话状态】\\n用户为新用户，尚未确认关键信息，需根据提问逐步补充。&quot;</span>
        <span class="token punctuation">}</span><span class="token punctuation">)</span>

    <span class="token comment"># 3. 最近N轮对话层：控制长度，保留最近5-10轮，避免冗余</span>
    <span class="token comment"># 动态截断逻辑：优先保留最近5轮，若token数仍超标，再缩减至3轮</span>
    max_recent_rounds <span class="token operator">=</span> <span class="token number">5</span>
    truncated_messages <span class="token operator">=</span> recent_messages<span class="token punctuation">[</span><span class="token operator">-</span>max_recent_rounds<span class="token punctuation">:</span><span class="token punctuation">]</span>
    <span class="token comment"># （实际工程中需添加token计数逻辑，此处简化）</span>
    messages<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>truncated_messages<span class="token punctuation">)</span>

    <span class="token comment"># 4. 检索知识注入层：按需注入，用system角色标注，明确为参考资料</span>
    <span class="token keyword">if</span> retrieved_docs<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 格式规范：标注来源+核心内容，便于模型引用</span>
        formatted_docs <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f&quot;【参考知识】\\n</span><span class="token interpolation"><span class="token punctuation">{</span>retrieved_docs<span class="token punctuation">}</span></span><span class="token string">\\n请严格基于上述知识回答，标注对应来源。&quot;</span></span>
        messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{</span>
            <span class="token string">&quot;role&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;system&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;content&quot;</span><span class="token punctuation">:</span> formatted_docs
        <span class="token punctuation">}</span><span class="token punctuation">)</span>

    <span class="token comment"># 工程关键：返回前检查总token数，确保不超过模型窗口上限</span>
    <span class="token comment"># （实际工程中需添加token计算和截断逻辑，优先砍检索知识和最近对话）</span>
    <span class="token keyword">return</span> messages
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>说明：</p><ol><li><strong>角色选择技巧</strong>：系统约束和状态摘要用<code>system</code>角色，而不是<code>user</code>或<code>assistant</code>——因为<code>system</code>角色的信息在模型处理中优先级更高，能有效抵抗注意力衰减；</li><li><strong>动态截断逻辑</strong>：最近对话的“N轮”不是固定值，而是根据token数动态调整，避免硬编码导致的窗口溢出；</li><li><strong>异常处理</strong>：考虑到“首次对话无状态”“无检索知识”等场景，补充默认逻辑，避免模型因输入不完整而产生幻觉；</li><li><strong>格式规范</strong>：每层信息都有明确的标签（【系统约束】【会话状态】），帮助模型区分不同类型的信息，减少混淆。</li></ol><p>重点注意，这段代码真正重要的不是“怎么写”，而是它体现的原则：</p><blockquote><p><strong>上下文是被设计出来的系统结构，而不是副产品。</strong></p></blockquote><hr><h3 id="_9-8-本章小结-上下文工程决定系统-能跑多远" tabindex="-1"><a class="header-anchor" href="#_9-8-本章小结-上下文工程决定系统-能跑多远" aria-hidden="true">#</a> 9.8 本章小结：上下文工程决定系统“能跑多远”</h3><p>通过这一章，你应该已经形成这样的核心认知：</p><ul><li>多轮对话失控不是偶然，也不是“Prompt 技巧不足”，而是<strong>上下文没有被当作一等工程对象来设计</strong>——没有分层、没有优先级、没有动态管理，让低价值信息挤占了高价值信息的生存空间；</li><li>上下文工程的本质是“信息治理”：通过分层设计，让系统约束“不被遗忘”、对话共识“不被稀释”、冗余信息“不被保留”、外部知识“按需注入”；</li><li>落地上下文工程的关键，是配套三大机制：<strong>分层信息定义机制</strong>（明确每层内容）、<strong>状态动态更新机制</strong>（每轮刷新摘要）、<strong>长度监控截断机制</strong>（避免窗口溢出）。</li></ul><p>但你也应该意识到一个新的边界：</p><blockquote><p>即使上下文被精心管理，系统依然只能回答“模型已知或上下文已提供”的内容。</p></blockquote><p>当用户的问题<strong>超出企业文档覆盖范围，或需要实时数据支撑</strong>（比如“当前我的报销申请审批到哪一步了”）时，仅靠上下文工程无法解决——此时需要引入“外部工具”和“检索增强”，让系统具备“主动获取信息”的能力。</p><p>下一部分，我们将聚焦 RAG（检索增强生成）与工具调用，探讨如何让企业知识库助手从“只能回答已知问题”，升级为“能解决未知问题”。</p>`,26);function m(k,v){const s=o("Mermaid");return e(),i("div",null,[r,a(s,{id:"mermaid-175",code:"eJyVVFtP21Ycf++nsFT1qaS5kSmypkptWadp3TQV3qI9mPgEohob2c6APZlLIKFJGsZlXJOyAs4q1WmBksRJyYdpzvHJE/0IPceOnaY3CfvJPuf/u/xvEzI3PcmMjdxgyHPrFoOe12DrGTR2O43sVWsXnbatjQrMLFtvmt10HmdPr1o52DJhbguvLlgLDZgzyakdHRc4RRkBCSYpTqfU3yUeMImkILA3QTARSYAhRZWlJ4C9GQhFo3yw9+mbSfLqJBuanh2SpZTIA54NBqZnBwHjkgw+wUvEwTCIe3jxUDAaGR/EC3+GN5SQRNU3A5ITkyo7Lgn8IIEgTH2qN5qIgKiHH45GQdil+4ZeB19J/g3Y4PDn8kVJHZDP0deDH+fp68LznDLJyTI3x4aZsEcRJZBuhfDlOkwfO9mnFaq8hMU12LiAmX9JbbC+jPUsrDYs/bUdcS/W3Wyj/XKnnofPtq3dJeusaTXLlnmCDso/jsv+u6NzigqmmAeSSDRwSVFV7N9XrQzW/yEV92N9CWZ2/OhgsbtTvGpl/2RZtl9jn+8u8zDWqa926k/R1goqLcGmCXcqNggBVcGsytxPJQUeyC6w9Wqvq2Vvo81zWMzcHiPGRVTQYeaih+4W3HZwP4ZrFfzOIKZw9YA6WL1A2jxa28Yn8w6LLCmKT03JIjOqcipgRlNTU5w859Kh53XYXrSa69gw/Wj/JTRyKFPsmE+7hySFX/NjEz+IoX0Nt9ccYrichkbDhnwM4kBUacb+ArLCqUlJ9JydXlqGFvEFA0QxqYdji5QAmiffoRqJwaOt7mIFnVVIcanH8jGuLvth7Rima0Q5Ku3bFD/NqkAWOYH5VZRmBMBPAP+YJAnMY6CkhH7l0OZrlDeguY4ozn/+e3/8gtsbcK/kpyU6fGFlV5Dx9huC+ruApo3224pJOy23Qjrt0aPf7POH9uWfY+TbIy0co40LXMqTosJigbigid48dzq2R9YbtX47HxKHaeIb6y8ICWlqtF1wmDvNGtRa9kU6QrEP5fVcT1RXa+LLNXIfHu0is9hpH6L5qsNMh8D4n8iC6RbJ3Xst5/UmbpfweRVp+nst36kXyJGTdSfZePEdXN0jR1SmO7Ou1zvELP3HeMIt4xU8Onf2JRViL0X4ZsEydZg+I1NmXxSS4pNRdU4ATIBxJ/6HIH0HF0rwTqS3NvoRwWtHhK4dEb52xLAX8bXFG6IRXy6zALF/4yOJBPLy"}),u,a(s,{id:"mermaid-451",code:"eJytVU1P21gU3fdXPKnqKk2HJGQUWaNK0yJGo2G6KN1FszDxC1j12JFjCszKfIQEJnwNlIEMIdBCE1UiESXDR+wh/6Xj9+ys6E/ofY7jNLRUXRBv8t7zPefcd8+9HlX51Bh6NnAHwe/ePUT3zoi5QqoF62LhyizQd017o0Jy8/ax0cosOQvvrsy8ZW6R/KZtzDuLMyTfgFM3OiHx6fQATiJRTo1rTxQBo6QoSdxdHEpGk/h+WlOV55i72xeOxYSQtwxOiII2xoVTk/dVZVwWsMCF+mCRVGQtmBb/wFwokprsxU8oKv4EPpnA/TjhwyfCoVh0pBceIL4AP4HF0TGNG1Ek4VO+/ut8yrh2LaFYMopjPmMkFsORjoBvSOgzAlnRehLi2ePDjwjs6cALfHqMV1V+iougiE8R62UA9judijqX6yRzCKWyZy5YRStvyeoauTglub+hlsRsQC3f6zvkoECOZ+xG+b1epHMlu3ToAvwYt86XyMqWfWLYRsluvKHFErz3w4j63cPhqbSGf0ePFRm08aKsoSF+Cqvu2ZWZe6pIOED3zklz1inPkdw20Nn7VatZtCsn4DCql6/Mhd84jusaJhh8iAaBctE6/5NuZunuHDEaZLviYgKRhic19GhclIQuD31ZJ6s5YNpvvc0HnsElyXS5THKnHnrHLm4+j+KWWXBqRXvxlOrTdG3LeTPt54PTaVGR0bDGa7g3F2gCmjuzmvt0uhYgtQtAIJljpzbP2qFZbBV2nOauU6/dmJNL/jhOd3SnuYaeIOe/qgfjkT/FCQw3CDm+wGqa15iQHgnRYKiPRWVOWhtVq3FgNf5qk9svt53aEXk1+xXmgTh9rdv1V1BWEE1PKmCJLrOmivgFL6FfZGVCwsLoteTpRtM+2rQXsrT6b4AWD2ljle5lAQT46coqWcqS7DxU9ubk/emyo9tGjjky22Be3FpmYfksOBINDf3qedV9e9AN/SnOtrthmUNX1KAog1x29DNj8qtkrDPH5DcDZKUGHkCuGZCTnyX/1D1p3WbuqnI9Cvk45degCvoEhPn94Owugb0s44zophvBujX+obSe9wJbuuFcrkGgH9L2CcxFWLbvmnVaxmztX/rW/1+f9i0D//0bZBo786BzEQ/gJtgeQr5mu3pEDurtac2o3TZus4NDoFut82VP3vqlc1Zz4yRRfj6sTUkY9aHOdPk+xJ7e4RV6EIV54m1BEE7wKc4dNddgQrcDE74dmMjtwPT7MF/6lIRvhvkI3QZbXA=="}),d])}const b=t(c,[["render",m],["__file","14.上下文工程.html.vue"]]);export{b as default};
